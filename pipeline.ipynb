{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMbZ033Ya4/bel6DYS2Dtj8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArnavMehrotra/ArNet/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports and Shared Object Creation\n"
      ],
      "metadata": {
        "id": "PVCn8-bbvhLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "if [ ! -d ArNet ]; then\n",
        "  git clone https://github.com/ArnavMehrotra/ArNet\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyZo6ila24p",
        "outputId": "9de39a91-4416-4b15-e5a9-7595f2cc31d0"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd ArNet/\n",
        "git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjlj2_BWhUEK",
        "outputId": "e3982c50-5eeb-46e5-8549-d2ebeed3ecee"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import ctypes\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "MAX_SIZE = 4096 * 2"
      ],
      "metadata": {
        "id": "WKg6NOaFzgIu"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7ZAvoAXddHJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf *.so\n",
        "so_name = f\"ops_{int(time.time())}.so\"\n",
        "!nvcc -Xcompiler -fPIC -shared -gencode arch=compute_80,code=sm_80 -o {so_name} ArNet/launch.cu ArNet/kernels.cu ArNet/pipeline.cu\n",
        "LIB = ctypes.CDLL(f\"./{so_name}\")"
      ],
      "metadata": {
        "id": "c8C6N1fku3GG"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CUDA Wrappers"
      ],
      "metadata": {
        "id": "ZrziYeFxvOuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gemmInt(a: np.array, b: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if(m != k):\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * n\n",
        "  op1 = np.array(a, dtype=np.int32)\n",
        "  op2 = np.array(b, dtype=np.int32)\n",
        "\n",
        "  out = np.zeros(N, dtype=np.int32)\n",
        "\n",
        "  lib.launchMultInt.argtypes =  [ctypes.POINTER(ctypes.c_int),\n",
        "                              ctypes.POINTER(ctypes.c_int),\n",
        "                              ctypes.POINTER(ctypes.c_int),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = op1.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
        "  b_ptr = op2.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
        "\n",
        "  lib.launchMultInt(a_ptr, b_ptr, c_ptr, j, k, m, n)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, n)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def gemm(a: np.array, b: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "\n",
        "  if(a.dtype != np.float32 or b.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if(m != k):\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * n\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchMult.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchMult(a_ptr, b_ptr, c_ptr, j, k, m, n)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, n)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "\n",
        "def gemm2(a: np.array, b: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "  if(a.dtype != np.float32 or b.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if(m != k):\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * n\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchMult2.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchMult2(a_ptr, b_ptr, c_ptr, j, k, m, n)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, n)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def gradient(a: np.array, y: np.array, lib: ctypes.CDLL):\n",
        "  if a.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "    return\n",
        "\n",
        "  if y.dtype != np.uint32:\n",
        "    print(\"label index type must be uint32\")\n",
        "    return\n",
        "\n",
        "  j, k = a.shape\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchGradient.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                                  ctypes.POINTER(ctypes.c_uint32),\n",
        "                                  ctypes.POINTER(ctypes.c_float),\n",
        "                                  ctypes.c_int,\n",
        "                                  ctypes.c_int]\n",
        "\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  y_ptr = y.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchGradient(a_ptr, y_ptr, b_ptr, j, k)\n",
        "\n",
        "  b_np = np.ctypeslib.as_array(b_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return b_np\n",
        "\n",
        "def biasAdd(a: np.array, b: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "  if a.dtype != np.float32 or b.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  n = b.shape[0]\n",
        "\n",
        "  if k != n:\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchBiasAdd.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchBiasAdd(a_ptr, b_ptr, c_ptr, j, k)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def scalarAdd(a: np.array, s: float, lib: ctypes.CDLL) -> np.array:\n",
        "  if a.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchScalarAdd.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_float,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchScalarAdd(a_ptr, c_ptr, s, N)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def matAdd(a: np.array, b: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "\n",
        "  if(a.dtype != np.float32 or b.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if m != j or n != k:\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchAdd.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchAdd(a_ptr, b_ptr, c_ptr, j, k)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def relu(a: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "\n",
        "  if(a.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchRelu.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchRelu(a_ptr, b_ptr, N)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(b_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def softmax(a: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "  if a.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchSoftmax.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                                 ctypes.POINTER(ctypes.c_float),\n",
        "                                 ctypes.c_int,\n",
        "                                 ctypes.c_int]\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchSoftmax(a_ptr, b_ptr, j, k)\n",
        "  c_np = np.ctypeslib.as_array(b_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np"
      ],
      "metadata": {
        "id": "dbgwZoPPul6c"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Functions"
      ],
      "metadata": {
        "id": "oetGfuJiv4l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PRINT = False\n",
        "\n",
        "def test_gemm(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "  m = k\n",
        "  n = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  hi = np.random.rand(j, k).astype(np.float32) * 100\n",
        "  hello = np.random.rand(m, n).astype(np.float32) * 100\n",
        "\n",
        "  start = time.time()\n",
        "  correct = hi @ hello\n",
        "  end = time.time()\n",
        "\n",
        "  if PRINT: print(f\"Total time for numpy: {(end - start)*1000:.3f} ms\")\n",
        "\n",
        "  start = time.time()\n",
        "  test = gemm(hi, hello, lib)\n",
        "  end = time.time()\n",
        "\n",
        "  if PRINT: print(f\"Total time for young arn: {(end - start)*1000:.3f} ms\")\n",
        "\n",
        "  start = time.time()\n",
        "  test2 = gemm2(hi, hello, lib)\n",
        "  end = time.time()\n",
        "\n",
        "  if PRINT:\n",
        "    print(f\"Total time for young arn (optimized): {(end - start)*1000:.3f} ms\")\n",
        "    print(\"gemm\")\n",
        "    print(f\"input 1: {j}x{k} matrix\")\n",
        "    print(f\"input 2: {m}x{n} matrix\")\n",
        "    print(f\"output: {j}x{n} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3) and np.allclose(correct, test2, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_biasAdd(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  b = np.random.rand(k).astype(np.float32)\n",
        "\n",
        "  correct = a + b\n",
        "  test = biasAdd(a, b, lib)\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"bias add\")\n",
        "    print(f\"input 1: {j}x{k} matrix\")\n",
        "    print(f\"input 2: 1x{k} matrix\")\n",
        "    print(f\"output: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_scalarAdd(lib: ctypes.CDLL):\n",
        "\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  s = np.random.rand()\n",
        "\n",
        "  correct = a + s\n",
        "  test = scalarAdd(a, s, lib)\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"scalar add\")\n",
        "    print(f\"input: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_matAdd(lib: ctypes.CDLL):\n",
        "\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  b = np.random.rand(j, k).astype(np.float32)\n",
        "\n",
        "  correct = a + b\n",
        "  test = matAdd(a, b, lib)\n",
        "\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"mat add\")\n",
        "    print(f\"input 1: {j}x{k} matrix\")\n",
        "    print(f\"input 2: {j}x{k} matrix\")\n",
        "    print(f\"output: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_relu(lib: ctypes.CDLL):\n",
        "\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.randn(j, k).astype(np.float32)\n",
        "\n",
        "  correct = np.maximum(a, 0)\n",
        "  test = relu(a, lib)\n",
        "\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"relu\")\n",
        "    print(f\"input: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def numpy_softmax(Z):\n",
        "    Z_stable = Z - np.max(Z, axis=1, keepdims=True)\n",
        "    exp_Z = np.exp(Z_stable)\n",
        "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "\n",
        "def test_softmax(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.randn(j, k).astype(np.float32) * 100\n",
        "\n",
        "  test = softmax(a, lib)\n",
        "\n",
        "  check = numpy_softmax(a)\n",
        "\n",
        "  good = np.allclose(test, check, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"softmax\")\n",
        "    print(f\"input: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_gradient(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  y = np.random.randint(0, k, size=(j, 1)).astype(np.uint32)\n",
        "\n",
        "  test = gradient(a, y, lib)\n",
        "\n",
        "  check = numpy_softmax(a)\n",
        "  check[np.arange(j), y.squeeze()] -= 1\n",
        "\n",
        "  good = np.allclose(test, check, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"gradient\")\n",
        "    print(f\"input 1: {j}x{k} matrix\")\n",
        "    print(f\"input 2: {j}x1 matrix\")\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_mlp(lib: ctypes.CDLL):\n",
        "    j, k = 4, 8\n",
        "    m, n = 16, 3\n",
        "\n",
        "    x = np.random.rand(j, k).astype(np.float32) * 10\n",
        "    w1 = np.random.rand(k, m).astype(np.float32)\n",
        "    b1 = np.random.rand(m).astype(np.float32) * 10\n",
        "    w2 = np.random.rand(m, n).astype(np.float32)\n",
        "    b2 = np.random.rand(n).astype(np.float32) * 10\n",
        "\n",
        "    out = np.zeros(j * n, dtype=np.float32)\n",
        "\n",
        "    lib.forward_pass.argtypes = [\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int\n",
        "    ]\n",
        "\n",
        "    out_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "    lib.forward_pass(x.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    w1.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    b1.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    w2.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    b2.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    out_ptr, j, k, m, n)\n",
        "\n",
        "    test = np.ctypeslib.as_array(out_ptr, (j, n)).reshape(j, n)\n",
        "    correct = numpy_softmax(np.maximum(x @ w1 + b1, 0) @ w2 + b2)\n",
        "\n",
        "    good = np.allclose(test, correct, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "    return good\n",
        "\n",
        "def run_tests(lib: ctypes.CDLL):\n",
        "    assert test_gemm(lib), \"gemm failed\"\n",
        "    assert test_matAdd(lib), \"matAdd failed\"\n",
        "    assert test_scalarAdd(lib), \"scalarAdd failed\"\n",
        "    assert test_relu(lib), \"relu failed\"\n",
        "    assert test_softmax(lib), \"softmax failed\"\n",
        "    assert test_gradient(lib), \"gradient failed\"\n",
        "    assert test_biasAdd(lib), \"biasAdd failed\"\n",
        "    assert test_mlp(lib), \"mlp failed\"\n",
        "\n",
        "    print(\"All tests passed!!\")"
      ],
      "metadata": {
        "id": "6c50RY_huap4"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Usage"
      ],
      "metadata": {
        "id": "BfO2yA3rvbGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_tests(LIB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmVkF5LY1iE5",
        "outputId": "50171b0e-9925-4371-dda0-d59e1dd53397"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!!\n"
          ]
        }
      ]
    }
  ]
}