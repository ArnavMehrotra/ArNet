{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPv/vXW84INJhnJ1DQiY1QD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArnavMehrotra/ArNet/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "if [ ! -d ArNet ]; then\n",
        "  git clone https://github.com/ArnavMehrotra/ArNet\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyZo6ila24p",
        "outputId": "023a0b52-2daf-456a-86eb-a648699e5d34"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import ctypes\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "max_size = 9000"
      ],
      "metadata": {
        "id": "WKg6NOaFzgIu"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7ZAvoAXddHJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd ArNet/\n",
        "git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjlj2_BWhUEK",
        "outputId": "49366cae-29ab-4fee-82ee-bf438a48cd8b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects:  25% (1/4)\u001b[K\rremote: Counting objects:  50% (2/4)\u001b[K\rremote: Counting objects:  75% (3/4)\u001b[K\rremote: Counting objects: 100% (4/4)\u001b[K\rremote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects:  33% (1/3)\u001b[K\rremote: Compressing objects:  66% (2/3)\u001b[K\rremote: Compressing objects: 100% (3/3)\u001b[K\rremote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 4.24 KiB | 4.24 MiB/s, done.\n",
            "From https://github.com/ArnavMehrotra/ArNet\n",
            "   962e158..1bbeee8  main       -> origin/main\n",
            "Updating 962e158..1bbeee8\n",
            "Fast-forward\n",
            " pipeline.ipynb | 833 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 1 file changed, 833 insertions(+)\n",
            " create mode 100644 pipeline.ipynb\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf *.so\n",
        "so_name = f\"ops_{int(time.time())}.so\"\n",
        "!nvcc -Xcompiler -fPIC -shared -gencode arch=compute_80,code=sm_80 -o {so_name} ArNet/launch.cu ArNet/kernels.cu\n",
        "lib = ctypes.CDLL(f\"./{so_name}\")"
      ],
      "metadata": {
        "id": "c8C6N1fku3GG"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemmInt(a: np.array, b: np.array) -> np.array:\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if(m != k):\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * n\n",
        "  op1 = np.array(a, dtype=np.int32)\n",
        "  op2 = np.array(b, dtype=np.int32)\n",
        "\n",
        "  out = np.zeros(N, dtype=np.int32)\n",
        "\n",
        "  lib.launchMultInt.argtypes =  [ctypes.POINTER(ctypes.c_int),\n",
        "                              ctypes.POINTER(ctypes.c_int),\n",
        "                              ctypes.POINTER(ctypes.c_int),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = op1.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
        "  b_ptr = op2.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
        "\n",
        "  lib.launchMultInt(a_ptr, b_ptr, c_ptr, j, k, m, n)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, n)\n",
        "\n",
        "  return c_np"
      ],
      "metadata": {
        "id": "9ulCbcNc5A7g"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemm(a: np.array, b: np.array) -> np.array:\n",
        "\n",
        "  if(a.dtype != np.float32 or b.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if(m != k):\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * n\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchMult.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchMult(a_ptr, b_ptr, c_ptr, j, k, m, n)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, n)\n",
        "\n",
        "  return c_np"
      ],
      "metadata": {
        "id": "sBIwHv00mZJY"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemm2(a: np.array, b: np.array) -> np.array:\n",
        "\n",
        "  lib = ctypes.CDLL(os.environ[\"LIB\"])\n",
        "\n",
        "  print(os.environ[\"LIB\"])\n",
        "  if(a.dtype != np.float32 or b.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if(m != k):\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * n\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchMult2.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchMult2(a_ptr, b_ptr, c_ptr, j, k, m, n)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, n)\n",
        "\n",
        "  return c_np"
      ],
      "metadata": {
        "id": "HkRdRaW9eBaA"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def biasAdd(a: np.array, b: np.array) -> np.array:\n",
        "  if a.dtype != np.float32 or b.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  n = b.shape[0]\n",
        "\n",
        "  if k != n:\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchBiasAdd.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchBiasAdd(a_ptr, b_ptr, c_ptr, j, k)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n"
      ],
      "metadata": {
        "id": "Gr1L8-Ztl2NT"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scalarAdd(a: np.array, s: float) -> np.array:\n",
        "  if a.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchScalarAdd.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_float,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchScalarAdd(a_ptr, c_ptr, s, N)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np"
      ],
      "metadata": {
        "id": "A5VbCYX8lPzF"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matAdd(a: np.array, b: np.array) -> np.array:\n",
        "\n",
        "  if(a.dtype != np.float32 or b.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if m != j or n != k:\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchAdd.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchAdd(a_ptr, b_ptr, c_ptr, j, k)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np"
      ],
      "metadata": {
        "id": "VMLbZLdbka5J"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(a: np.array) -> np.array:\n",
        "\n",
        "  if(a.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchRelu.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchRelu(a_ptr, b_ptr, N)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(b_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np"
      ],
      "metadata": {
        "id": "RsCMy5JeiPzQ"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(a: np.array) -> np.array:\n",
        "  if a.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchSoftmax.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                                 ctypes.POINTER(ctypes.c_float),\n",
        "                                 ctypes.c_int,\n",
        "                                 ctypes.c_int]\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchSoftmax(a_ptr, b_ptr, j, k)\n",
        "  c_np = np.ctypeslib.as_array(b_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np"
      ],
      "metadata": {
        "id": "9FYLLtE_XhET"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(a: np.array, y: np.array):\n",
        "  if a.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "    return\n",
        "\n",
        "  if y.dtype != np.uint32:\n",
        "    print(\"label index type must be uint32\")\n",
        "    return\n",
        "\n",
        "  j, k = a.shape\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchGradient.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                                  ctypes.POINTER(ctypes.c_uint32),\n",
        "                                  ctypes.POINTER(ctypes.c_float),\n",
        "                                  ctypes.c_int,\n",
        "                                  ctypes.c_int]\n",
        "\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  y_ptr = y.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchGradient(a_ptr, y_ptr, b_ptr, j, k)\n",
        "\n",
        "  b_np = np.ctypeslib.as_array(b_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return b_np"
      ],
      "metadata": {
        "id": "zar5Vmu4Clgl"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gemm():\n",
        "  j = np.random.randint(2, max_size)\n",
        "  k = np.random.randint(2, max_size)\n",
        "  m = k\n",
        "  n = np.random.randint(2, max_size)\n",
        "\n",
        "  hi = np.random.rand(j, k).astype(np.float32) * 100\n",
        "  hello = np.random.rand(m, n).astype(np.float32) * 100\n",
        "\n",
        "  start = time.time()\n",
        "  correct = hi @ hello\n",
        "  end = time.time()\n",
        "\n",
        "  print(f\"Total time for numpy: {(end - start)*1000:.3f} ms\")\n",
        "\n",
        "  start = time.time()\n",
        "  test = gemm(hi, hello)\n",
        "  end = time.time()\n",
        "\n",
        "  print(f\"Total time for young arn: {(end - start)*1000:.3f} ms\")\n",
        "\n",
        "  start = time.time()\n",
        "  test2 = gemm2(hi, hello)\n",
        "  end = time.time()\n",
        "\n",
        "  print(f\"Total time for young arn (optimized): {(end - start)*1000:.3f} ms\")\n",
        "\n",
        "  print(f\"input 1: {j}x{k} matrix\")\n",
        "  print(f\"input 2: {m}x{n} matrix\")\n",
        "  print(f\"output: {j}x{n} matrix\")\n",
        "\n",
        "\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3) and np.allclose(correct, test2, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  return good"
      ],
      "metadata": {
        "id": "_QsSj8kszUqf"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_biasAdd():\n",
        "  j = np.random.randint(2, max_size)\n",
        "  k = np.random.randint(2, max_size)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  b = np.random.rand(k).astype(np.float32)\n",
        "\n",
        "  correct = a + b\n",
        "  test = biasAdd(a, b)\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  print(f\"input 1: {j}x{k} matrix\")\n",
        "  print(f\"input 2: 1x{k} matrix\")\n",
        "  print(f\"output: {j}x{k} matrix\")\n",
        "\n",
        "  return good"
      ],
      "metadata": {
        "id": "3Lc32I_pUAfe"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_scalarAdd():\n",
        "\n",
        "  j = np.random.randint(2, max_size)\n",
        "  k = np.random.randint(2, max_size)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  s = np.random.rand()\n",
        "\n",
        "  correct = a + s\n",
        "  test = scalarAdd(a, s)\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "\n",
        "  return good"
      ],
      "metadata": {
        "id": "KVGRkn5zvzJF"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_matAdd():\n",
        "\n",
        "  j = np.random.randint(2, max_size)\n",
        "  k = np.random.randint(2, max_size)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  b = np.random.rand(j, k).astype(np.float32)\n",
        "\n",
        "  correct = a + b\n",
        "  test = matAdd(a, b)\n",
        "\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  print(f\"input 1: {j}x{k} matrix\")\n",
        "  print(f\"input 2: {j}x{k} matrix\")\n",
        "  print(f\"output: {j}x{k} matrix\")\n",
        "\n",
        "  return good"
      ],
      "metadata": {
        "id": "hxYD-VEEyYTp"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_relu():\n",
        "\n",
        "  j = np.random.randint(2, max_size)\n",
        "  k = np.random.randint(2, max_size)\n",
        "\n",
        "  a = np.random.randn(j, k).astype(np.float32)\n",
        "\n",
        "  correct = np.maximum(a, 0)\n",
        "  test = relu(a)\n",
        "\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  print(f\"input: {j}x{k} matrix\")\n",
        "\n",
        "  return good"
      ],
      "metadata": {
        "id": "KUhIcmRTQ6YW"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_softmax(Z):\n",
        "    Z_stable = Z - np.max(Z, axis=1, keepdims=True)\n",
        "    exp_Z = np.exp(Z_stable)\n",
        "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "\n",
        "def test_softmax():\n",
        "  sz = max_size\n",
        "\n",
        "  j = np.random.randint(2, sz)\n",
        "  k = np.random.randint(2, sz)\n",
        "\n",
        "  a = np.random.randn(j, k).astype(np.float32) * 100\n",
        "\n",
        "  test = softmax(a)\n",
        "\n",
        "  check = numpy_softmax(a)\n",
        "\n",
        "  good = np.allclose(test, check, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_gradient():\n",
        "  j = np.random.randint(2, max_size)\n",
        "  k = np.random.randint(2, max_size)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  y = np.random.randint(0, k, size=(j, 1)).astype(np.uint32)\n",
        "\n",
        "  test = gradient(a, y)\n",
        "\n",
        "  check = numpy_softmax(a)\n",
        "  check[np.arange(j), y.squeeze()] -= 1\n",
        "\n",
        "  good = np.allclose(test, check, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  return good"
      ],
      "metadata": {
        "id": "vCdoH1kVX7YY"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not test_biasAdd()): print(\"biasAdd failed\")\n",
        "if(not test_gemm()): print(\"gemm failed\")\n",
        "if(not test_matAdd()): print(\"matAdd failed\")\n",
        "if(not test_scalarAdd()): print(\"scalarAdd failed\")\n",
        "if(not test_relu()): print(\"relu failed\")\n",
        "if(not test_softmax()): print(\"softmax failed\")\n",
        "if(not test_gradient()): print(\"gradient failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmVkF5LY1iE5",
        "outputId": "32b509fb-654f-4324-b40f-dec73afe696f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input 1: 6372x1610 matrix\n",
            "input 2: 1x1610 matrix\n",
            "output: 6372x1610 matrix\n",
            "Total time for numpy: 160.584 ms\n",
            "Total time for young arn: 83.243 ms\n",
            "./ops_1754289890.so\n",
            "Total time for young arn (optimized): 130.152 ms\n",
            "input 1: 3602x4781 matrix\n",
            "input 2: 4781x2557 matrix\n",
            "output: 3602x2557 matrix\n",
            "input 1: 5527x6133 matrix\n",
            "input 2: 5527x6133 matrix\n",
            "output: 5527x6133 matrix\n",
            "input: 4583x2566 matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lr = 0.001\n",
        "# epochs = 100\n",
        "\n",
        "# J, D, H, K = 64, 32, 16, 4  # batch size, input dim, hidden dim, output classes\n",
        "\n",
        "# X = np.random.randn(J, D).astype(np.float32)\n",
        "# Y = np.random.randint(0, K, size=(J, 1)).astype(np.uint32)\n",
        "\n",
        "# W1 = np.random.randn(D, H).astype(np.float32) * 0.01\n",
        "# b1 = np.zeros((H,), dtype=np.float32)\n",
        "\n",
        "# W2 = np.random.randn(H, K).astype(np.float32) * 0.01\n",
        "# b2 = np.zeros((K,), dtype=np.float32)\n",
        "# for i in range(epochs):\n",
        "#   #forward pass\n",
        "#   M1 = gemm(X, W1)\n",
        "#   Z1 = biasAdd(M1, b1)\n",
        "#   A1 = relu(Z1)\n",
        "#   M2 = gemm(A1, W2)\n",
        "#   Z2 = biasAdd(M2, b2)\n",
        "#   A2 = softmax(Z2)\n",
        "#   probs = A2[np.arange(J), Y.squeeze()]\n",
        "#   loss = -np.log(probs + 1e-8).mean()\n",
        "\n",
        "#   print(loss)\n",
        "\n",
        "#   #backprop\n",
        "#   dZ2 = gradient(A2, Y)\n",
        "#   dW2 = gemm(A1.T, dZ2)\n",
        "#   db2 = sum(dZ2)\n",
        "#   dA1 = gemm(dZ2, W2.T)\n",
        "#   dZ1 = dA1 * (Z1 > 0)\n",
        "#   dW1 = gemm(X.T, dZ1)\n",
        "#   db1 = sum(dZ1)\n",
        "\n",
        "#   #update weights\n",
        "#   W1 -= lr * dW1\n",
        "#   b1 -= lr * db1\n",
        "\n",
        "#   W2 -= lr * dW2\n",
        "#   b2 -= lr * db2"
      ],
      "metadata": {
        "id": "kUJ79u2koYg6"
      },
      "execution_count": 109,
      "outputs": []
    }
  ]
}