{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYEhomcKWaDA09694k5hbc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArnavMehrotra/ArNet/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports and Shared Object Creation\n"
      ],
      "metadata": {
        "id": "PVCn8-bbvhLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "if [ ! -d ArNet ]; then\n",
        "  git clone https://github.com/ArnavMehrotra/ArNet\n",
        "fi\n",
        "cd ArNet/\n",
        "git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyZo6ila24p",
        "outputId": "3262eb49-1808-406b-b616-4c373422776a"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 386 bytes | 386.00 KiB/s, done.\n",
            "From https://github.com/ArnavMehrotra/ArNet\n",
            "   6eca310..882b29e  main       -> origin/main\n",
            "Updating 6eca310..882b29e\n",
            "Fast-forward\n",
            " op.h        | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " pipeline.cu | 9 \u001b[31m---------\u001b[m\n",
            " 2 files changed, 1 insertion(+), 10 deletions(-)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import ctypes\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# MAX_SIZE = 4096 * 2\n",
        "MAX_SIZE = 16\n",
        "PRINT = False"
      ],
      "metadata": {
        "id": "WKg6NOaFzgIu"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7ZAvoAXddHJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf *.so\n",
        "so_name = f\"ops_{int(time.time())}.so\"\n",
        "!nvcc -Xcompiler -fPIC -shared -gencode arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -o {so_name} ArNet/launch.cu ArNet/kernels.cu ArNet/pipeline.cu\n",
        "LIB = ctypes.CDLL(f\"./{so_name}\")"
      ],
      "metadata": {
        "id": "c8C6N1fku3GG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efabbe0d-e80d-462d-e5c4-78714d02f391"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mArNet/kernels.cu(272)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity s_data is treated as a static definition\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mArNet/kernels.cu(71)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity a is treated as a static definition\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mArNet/kernels.cu(72)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity b is treated as a static definition\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mArNet/kernels.cu(272)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity s_data is treated as a static definition\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mArNet/kernels.cu(71)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity a is treated as a static definition\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mArNet/kernels.cu(72)\u001b[0m: \u001b[01;35mwarning\u001b[0m #20044-D: extern declaration of the entity b is treated as a static definition\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CUDA Wrappers"
      ],
      "metadata": {
        "id": "ZrziYeFxvOuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gemm2(a: np.array, b: np.array, lib: ctypes.CDLL, backward: bool) -> tuple():\n",
        "  if(a.dtype != np.float32 or b.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  N = j * n\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchMult2.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_bool]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchMult2(a_ptr, b_ptr, c_ptr, j, k, m, n, backward)\n",
        "\n",
        "  a_np = np.ctypeslib.as_array(a_ptr, (j, k))\n",
        "  b_np = np.ctypeslib.as_array(b_ptr, (m, n))\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (j, n))\n",
        "\n",
        "  return c_np, a_np, b_np\n",
        "\n",
        "\n",
        "def gemmInt(a: np.array, b: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if(m != k):\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * n\n",
        "  op1 = np.array(a, dtype=np.int32)\n",
        "  op2 = np.array(b, dtype=np.int32)\n",
        "\n",
        "  out = np.zeros(N, dtype=np.int32)\n",
        "\n",
        "  lib.launchMultInt.argtypes =  [ctypes.POINTER(ctypes.c_int),\n",
        "                              ctypes.POINTER(ctypes.c_int),\n",
        "                              ctypes.POINTER(ctypes.c_int),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = op1.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
        "  b_ptr = op2.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_int))\n",
        "\n",
        "  lib.launchMultInt(a_ptr, b_ptr, c_ptr, j, k, m, n)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, n)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "\n",
        "def gemm(a: np.array, b: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "\n",
        "  if(a.dtype != np.float32 or b.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if(m != k):\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * n\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchMult.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchMult(a_ptr, b_ptr, c_ptr, j, k, m, n)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, n)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "\n",
        "def gradient(a: np.array, y: np.array, lib: ctypes.CDLL):\n",
        "  if a.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "    return\n",
        "\n",
        "  if y.dtype != np.uint32:\n",
        "    print(\"label index type must be uint32\")\n",
        "    return\n",
        "\n",
        "  j, k = a.shape\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchGradient.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                                  ctypes.POINTER(ctypes.c_uint32),\n",
        "                                  ctypes.POINTER(ctypes.c_float),\n",
        "                                  ctypes.c_int,\n",
        "                                  ctypes.c_int]\n",
        "\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  y_ptr = y.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchGradient(a_ptr, y_ptr, b_ptr, j, k)\n",
        "\n",
        "  b_np = np.ctypeslib.as_array(b_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return b_np\n",
        "\n",
        "def biasAdd(a: np.array, b: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "  if a.dtype != np.float32 or b.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  n = b.shape[0]\n",
        "\n",
        "  if k != n:\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchBiasAdd.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchBiasAdd(a_ptr, b_ptr, c_ptr, j, k)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def scalarAdd(a: np.array, s: float, lib: ctypes.CDLL) -> np.array:\n",
        "  if a.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchScalarAdd.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_float,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchScalarAdd(a_ptr, c_ptr, s, N)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def matAdd(a: np.array, b: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "\n",
        "  if(a.dtype != np.float32 or b.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  m, n = b.shape\n",
        "\n",
        "  if m != j or n != k:\n",
        "    print(\"matrix dimensions do not match\")\n",
        "    return\n",
        "\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchAdd.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int,\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = b.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  c_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchAdd(a_ptr, b_ptr, c_ptr, j, k)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(c_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def relu(a: np.array, lib: ctypes.CDLL) -> np.ndarray:\n",
        "\n",
        "  if(a.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchRelu.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.POINTER(ctypes.c_float),\n",
        "                              ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchRelu(a_ptr, b_ptr, N)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(b_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def softmax(a: np.array, lib: ctypes.CDLL) -> np.array:\n",
        "  if a.dtype != np.float32:\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "  N = j * k\n",
        "\n",
        "  out = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "  lib.launchSoftmax.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                                 ctypes.POINTER(ctypes.c_float),\n",
        "                                 ctypes.c_int,\n",
        "                                 ctypes.c_int]\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchSoftmax(a_ptr, b_ptr, j, k)\n",
        "  c_np = np.ctypeslib.as_array(b_ptr, (N,)).reshape(j, k)\n",
        "\n",
        "  return c_np\n",
        "\n",
        "def sumCols(a: np.array, lib: ctypes.CDLL) -> np.ndarray:\n",
        "  if(a.dtype != np.float32):\n",
        "    print(\"data type must be float32\")\n",
        "\n",
        "  j, k = a.shape\n",
        "\n",
        "  out = np.zeros(k, dtype=np.float32)\n",
        "\n",
        "  lib.launchSumCols.argtypes =  [ctypes.POINTER(ctypes.c_float),\n",
        "                                ctypes.POINTER(ctypes.c_float),\n",
        "                                ctypes.c_int,\n",
        "                                ctypes.c_int]\n",
        "\n",
        "  a_ptr = a.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "  b_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "\n",
        "  lib.launchSumCols(a_ptr, b_ptr, j, k)\n",
        "\n",
        "  c_np = np.ctypeslib.as_array(b_ptr, (k,))\n",
        "\n",
        "  return c_np"
      ],
      "metadata": {
        "id": "dbgwZoPPul6c"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Functions"
      ],
      "metadata": {
        "id": "oetGfuJiv4l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_sumCols(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "\n",
        "  correct = np.sum(a, axis=0)\n",
        "  test = sumCols(a, lib)\n",
        "\n",
        "  if PRINT:\n",
        "    print(f\"expected:\\n{correct}\\ngot:\\n{test}\")\n",
        "\n",
        "  return np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "def test_gemm_backward(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "  m = k\n",
        "  n = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  hi = np.random.rand(j, k).astype(np.float32) * 10\n",
        "  hello = np.random.rand(m, n).astype(np.float32) * 10\n",
        "\n",
        "  correct_c = hi @ hello\n",
        "  correct_a = correct_c @ hello.T\n",
        "  correct_b = hi.T @ correct_c\n",
        "\n",
        "  test_c, test_a, test_b = gemm2(hi, hello, lib, True)\n",
        "\n",
        "  good_a = np.allclose(correct_a, test_a, rtol=1e-3, atol=1e-3)\n",
        "  good_b = np.allclose(correct_b, test_b, rtol=1e-3, atol=1e-3)\n",
        "  good_c = np.allclose(correct_c, test_c, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "\n",
        "  if PRINT:\n",
        "    print(f\"inputs: {j}x{k} and {m}x{n}\")\n",
        "    print(f\"expected:\\n{correct_c}\\ngot:\\n{test_c}\")\n",
        "\n",
        "  return good_a and good_c and good_b\n",
        "\n",
        "def test_gemm(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "  m = k\n",
        "  n = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  hi = np.random.rand(j, k).astype(np.float32) * 100\n",
        "  hello = np.random.rand(m, n).astype(np.float32) * 100\n",
        "\n",
        "  start = time.time()\n",
        "  correct = hi @ hello\n",
        "  end = time.time()\n",
        "\n",
        "  if PRINT: print(f\"Total time for numpy: {(end - start)*1000:.3f} ms\")\n",
        "\n",
        "  start = time.time()\n",
        "  test = gemm(hi, hello, lib)\n",
        "  end = time.time()\n",
        "\n",
        "  if PRINT: print(f\"Total time for young arn: {(end - start)*1000:.3f} ms\")\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"gemm\")\n",
        "    print(f\"input 1: {j}x{k} matrix\")\n",
        "    print(f\"input 2: {m}x{n} matrix\")\n",
        "    print(f\"output: {j}x{n} matrix\")\n",
        "    print(f\"expected:\\n{correct}\\ngot:\\n{test}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_biasAdd(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  b = np.random.rand(k).astype(np.float32)\n",
        "\n",
        "  correct = a + b\n",
        "  test = biasAdd(a, b, lib)\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"bias add\")\n",
        "    print(f\"input 1: {j}x{k} matrix\")\n",
        "    print(f\"input 2: 1x{k} matrix\")\n",
        "    print(f\"output: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_scalarAdd(lib: ctypes.CDLL):\n",
        "\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  s = np.random.rand()\n",
        "\n",
        "  correct = a + s\n",
        "  test = scalarAdd(a, s, lib)\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"scalar add\")\n",
        "    print(f\"input: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_matAdd(lib: ctypes.CDLL):\n",
        "\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  b = np.random.rand(j, k).astype(np.float32)\n",
        "\n",
        "  correct = a + b\n",
        "  test = matAdd(a, b, lib)\n",
        "\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"mat add\")\n",
        "    print(f\"input 1: {j}x{k} matrix\")\n",
        "    print(f\"input 2: {j}x{k} matrix\")\n",
        "    print(f\"output: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_relu(lib: ctypes.CDLL):\n",
        "\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.randn(j, k).astype(np.float32)\n",
        "\n",
        "  correct = np.maximum(a, 0)\n",
        "  test = relu(a, lib)\n",
        "\n",
        "  good = np.allclose(correct, test, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"relu\")\n",
        "    print(f\"input: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def numpy_softmax(Z):\n",
        "    Z_stable = Z - np.max(Z, axis=1, keepdims=True)\n",
        "    exp_Z = np.exp(Z_stable)\n",
        "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def test_softmax(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.randn(j, k).astype(np.float32) * 100\n",
        "\n",
        "  test = softmax(a, lib)\n",
        "\n",
        "  check = numpy_softmax(a)\n",
        "\n",
        "  good = np.allclose(test, check, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"softmax\")\n",
        "    print(f\"input: {j}x{k} matrix\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_gradient(lib: ctypes.CDLL):\n",
        "  j = np.random.randint(2, MAX_SIZE)\n",
        "  k = np.random.randint(2, MAX_SIZE)\n",
        "\n",
        "  a = np.random.rand(j, k).astype(np.float32)\n",
        "  y = np.random.randint(0, k, size=(j, 1)).astype(np.uint32)\n",
        "\n",
        "  test = gradient(a, y, lib)\n",
        "\n",
        "  check = numpy_softmax(a)\n",
        "  check[np.arange(j), y.squeeze()] -= 1\n",
        "\n",
        "  good = np.allclose(test, check, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "  if PRINT:\n",
        "    print(\"gradient\")\n",
        "    print(f\"input 1: {j}x{k} matrix\")\n",
        "    print(f\"input 2: {j}x1 matrix\")\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "  return good\n",
        "\n",
        "def test_mlp(lib: ctypes.CDLL):\n",
        "    j, k = 4, 8\n",
        "    m, n = 16, 3\n",
        "\n",
        "    x = np.random.rand(j, k).astype(np.float32) * 10\n",
        "    w1 = np.random.rand(k, m).astype(np.float32)\n",
        "    b1 = np.random.rand(m).astype(np.float32) * 10\n",
        "    w2 = np.random.rand(m, n).astype(np.float32)\n",
        "    b2 = np.random.rand(n).astype(np.float32) * 10\n",
        "    y = np.random.randint(0, n, size=(j, 1)).astype(np.uint32)\n",
        "\n",
        "    out = np.zeros(j * n, dtype=np.float32)\n",
        "\n",
        "\n",
        "    correct = numpy_softmax(np.maximum(x @ w1 + b1, 0) @ w2 + b2)\n",
        "    correct[np.arange(j), y.squeeze()] -= 1\n",
        "    z = np.maximum(x @ w1 + b1, 0)\n",
        "    correct_w2 =  z.T @ (correct)\n",
        "    correct_b2 = np.sum(correct, axis=0)\n",
        "\n",
        "    lib.test_layers.argtypes = [\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_uint32),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int\n",
        "    ]\n",
        "\n",
        "    out_ptr = out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
        "    lib.test_layers(x.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    w1.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    b1.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    w2.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    b2.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                    y.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32)),\n",
        "                    out_ptr, j, k, m, n)\n",
        "\n",
        "    test = np.ctypeslib.as_array(out_ptr, (j, n))\n",
        "    test_w2 = np.ctypeslib.as_array(w2.ctypes.data_as(ctypes.POINTER(ctypes.c_float)), (m, n))\n",
        "    test_b2 = np.ctypeslib.as_array(b2.ctypes.data_as(ctypes.POINTER(ctypes.c_float)), (n,))\n",
        "\n",
        "    good_y = np.allclose(test, correct, rtol=1e-3, atol=1e-3)\n",
        "    good_w2 = np.allclose(test_w2, correct_w2, rtol=1e-3, atol=1e-3)\n",
        "    good_b2 = np.allclose(test_b2, correct_b2, rtol=1e-3, atol=1e-3)\n",
        "\n",
        "    print(f\"y: {good_y}\")\n",
        "    print(f\"w2: {good_w2}\")\n",
        "    print(f\"b2: {good_b2}\")\n",
        "\n",
        "    print(f\"expected:\\n{correct_b2}\\ngot:\\n{test_b2}\")\n",
        "\n",
        "    good = good_y and good_w2 and good_b2\n",
        "\n",
        "    return good\n",
        "\n",
        "def run_tests(lib: ctypes.CDLL):\n",
        "    assert test_gemm(lib), \"gemm failed\"\n",
        "    assert test_matAdd(lib), \"matAdd failed\"\n",
        "    assert test_scalarAdd(lib), \"scalarAdd failed\"\n",
        "    assert test_relu(lib), \"relu failed\"\n",
        "    assert test_softmax(lib), \"softmax failed\"\n",
        "    assert test_gradient(lib), \"gradient failed\"\n",
        "    assert test_biasAdd(lib), \"biasAdd failed\"\n",
        "    assert test_sumCols(lib), \"sumCols failed\"\n",
        "    # assert test_mlp(lib), \"mlp failed\"\n",
        "    assert test_gemm_backward(lib), \"gemm_backward failed\"\n",
        "\n",
        "    print(\"All tests passed!!\")"
      ],
      "metadata": {
        "id": "6c50RY_huap4"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Usage"
      ],
      "metadata": {
        "id": "BfO2yA3rvbGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_tests(LIB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmVkF5LY1iE5",
        "outputId": "a66e1777-3d38-4a79-fbea-6135be4d9a50"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_mlp(LIB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YivKB5TdFlEY",
        "outputId": "c24eeb24-ac01-459e-9403-2b2539e78fb1"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: True\n",
            "w2: True\n",
            "b2: True\n",
            "expected:\n",
            "[-1.0000000e+00  1.0000000e+00  1.1057157e-08]\n",
            "got:\n",
            "[-1.0000000e+00  1.0000000e+00  1.1057159e-08]\n",
            "True\n"
          ]
        }
      ]
    }
  ]
}